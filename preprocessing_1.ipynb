{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 86bb5962-3edc-45db-b45e-71da2358c037)')' thrown while requesting GET https://huggingface.co/datasets/roman-bushuiev/MassSpecGym/resolve/main/data/MassSpecGym.tsv\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://datasets/roman-bushuiev/MassSpecGym/data/MassSpecGym.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstrument_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrbitrap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdrop_duplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minchikey\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m df\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:1013\u001b[0m, in \u001b[0;36mHfFileSystemFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# block_size=0 enables fast streaming\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mread(length)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/fsspec/spec.py:2083\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2083\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_fetch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m length)\n\u001b[1;32m   2085\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2087\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_log_stats(),\n\u001b[1;32m   2091\u001b[0m )\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/fsspec/caching.py:249\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    247\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_requested_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetcher(start, end)  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:969\u001b[0m, in \u001b[0;36mHfFileSystemFile._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    958\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39m_build_hf_headers(),\n\u001b[1;32m    961\u001b[0m }\n\u001b[1;32m    962\u001b[0m url \u001b[38;5;241m=\u001b[39m hf_hub_url(\n\u001b[1;32m    963\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[1;32m    964\u001b[0m     revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m    968\u001b[0m )\n\u001b[0;32m--> 969\u001b[0m r \u001b[38;5;241m=\u001b[39m http_backoff(\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m     url,\n\u001b[1;32m    972\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    973\u001b[0m     retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m502\u001b[39m, \u001b[38;5;241m503\u001b[39m, \u001b[38;5;241m504\u001b[39m),\n\u001b[1;32m    974\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_DOWNLOAD_TIMEOUT,\n\u001b[1;32m    975\u001b[0m )\n\u001b[1;32m    976\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    266\u001b[0m         req,\n\u001b[1;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[1;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[1;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/roman-bushuiev/MassSpecGym/data/MassSpecGym.tsv\", sep=\"\\t\")\n",
    "df = df[df[\"instrument_type\"] == \"Orbitrap\"].drop_duplicates([\"inchikey\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      unique_values  non_null_count    dtype\n",
      "identifier                    22646           22646   object\n",
      "mzs                           22604           22646   object\n",
      "intensities                   22463           22646   object\n",
      "smiles                        22642           22646   object\n",
      "inchikey                      22646           22646   object\n",
      "formula                       15225           22646   object\n",
      "precursor_formula             15795           22646   object\n",
      "parent_mass                   18152           22646  float64\n",
      "precursor_mz                  18084           22646  float64\n",
      "adduct                            2           22646   object\n",
      "instrument_type                   1           22646   object\n",
      "collision_energy               1895           15291  float64\n",
      "fold                              3           22646   object\n",
      "simulation_challenge              2           22646     bool\n"
     ]
    }
   ],
   "source": [
    "unique_counts = df.nunique()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"unique_values\": unique_counts,\n",
    "    \"non_null_count\": df.count(),\n",
    "    \"dtype\": df.dtypes\n",
    "})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "train    17220\n",
       "val       2775\n",
       "test      2651\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fold\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"mzs\"] = df[\"mzs\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "df[\"intensities\"] = df[\"intensities\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate by folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df[\"fold\"] == \"train\"]\n",
    "df_val = df[df[\"fold\"] == \"val\"]\n",
    "df_test = df[df[\"fold\"] == \"test\"]\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the preprocessed data\n",
    "df_train.to_csv(\"train_data.csv\", index=False)\n",
    "df_val.to_csv(\"val_data.csv\", index=False)\n",
    "df_test.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity scores using MatchMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_val = pd.read_csv(\"val_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df[\"mzs\"] = df[\"mzs\"].apply(lambda x: np.array(list(map(float, x.strip(\"[]\").split()))))\n",
    "    df[\"intensities\"] = df[\"intensities\"].apply(lambda x: np.array(list(map(float, x.strip(\"[]\").split()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matchms import Spectrum\n",
    "from matchms.similarity import ModifiedCosine\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFMCS\n",
    "\n",
    "def similarity_function(spectrum_1, spectrum_2):\n",
    "    \"\"\"\n",
    "    Calculate the modified cosine similarity between two spectra.\n",
    "    \"\"\"\n",
    "    # Use factory to construct a similarity function\n",
    "    modified_cosine = ModifiedCosine(tolerance=0.01)\n",
    "    score = modified_cosine.pair(spectrum_1, spectrum_2)\n",
    "    return float(score['score'])\n",
    "\n",
    "def compute_similarity(row, spectrum_ref):\n",
    "    spectrum_candidate = Spectrum(mz=row[\"mzs\"],\n",
    "                          intensities=row[\"intensities\"],\n",
    "                          metadata={\"precursor_mz\": row[\"precursor_mz\"]})\n",
    "    score = similarity_function(spectrum_ref, spectrum_candidate)\n",
    "    return score\n",
    "\n",
    "def compute_mcs_percent(row, mol_ref):\n",
    "    \"\"\"\n",
    "    MCS % based on number of atoms in the reference molecule.\n",
    "    \"\"\"\n",
    "    mol_candidate = Chem.MolFromSmiles(row[\"smiles\"])\n",
    "    mcs = rdFMCS.FindMCS([mol_candidate, mol_ref], completeRingsOnly=True)\n",
    "    if mcs.smartsString == \"\":\n",
    "        return 0.0\n",
    "\n",
    "    common = Chem.MolFromSmarts(mcs.smartsString)\n",
    "    common_atoms = common.GetNumAtoms()\n",
    "    total_atoms_ref = mol_ref.GetNumAtoms()\n",
    "\n",
    "    return (common_atoms / total_atoms_ref) * 100\n",
    "\n",
    "def select_most_similar_spectra(row, table, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Select the most similar spectra from the dataframe.\n",
    "    \"\"\"\n",
    "    df = table.copy()\n",
    "    spectrum = Spectrum(mz=row[\"mzs\"],\n",
    "                        intensities=row[\"intensities\"],\n",
    "                        metadata={\"precursor_mz\": row[\"precursor_mz\"]})\n",
    "    mol = Chem.MolFromSmiles(row[\"smiles\"])\n",
    "    df['matchms_similarity'] = df.apply(lambda r: compute_similarity(r, spectrum), axis=1)\n",
    "    df['mcs_percent'] = df.apply(lambda r: compute_mcs_percent(r, mol), axis=1)\n",
    "    df.loc[df['inchikey'] == row[\"inchikey\"], 'matchms_similarity'] = -1.0\n",
    "    df.loc[df['inchikey'] == row[\"inchikey\"], 'mcs_percent'] = -1.0\n",
    "    df[\"combined_score\"] = df[[\"matchms_similarity\", \"mcs_percent\"]].min(axis=1)\n",
    "    df.sort_values(by=\"combined_score\", ascending=False, inplace=True)\n",
    "    return pd.Series([df.iloc[0][\"inchikey\"], \n",
    "                      df.iloc[0][\"matchms_similarity\"], \n",
    "                      df.iloc[0][\"mcs_percent\"],\n",
    "                      df.iloc[0][\"combined_score\"]\n",
    "                    ])\n",
    "\n",
    "def similarity_search(df):\n",
    "    df[[\"inchikey_similar\",\n",
    "        \"matchms_similarity\",\n",
    "        \"mcs_percent\",\n",
    "        \"combined_score\"]] = df.apply(lambda row: select_most_similar_spectra(row, df), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matchms import Spectrum\n",
    "from matchms.similarity import ModifiedCosine\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to compute matchms similarity between two spectra\n",
    "def similarity_function(spectrum_1, spectrum_2):\n",
    "    modified_cosine = ModifiedCosine(tolerance=0.01)\n",
    "    score = modified_cosine.pair(spectrum_1, spectrum_2)\n",
    "    return float(score['score'])\n",
    "\n",
    "# Function to compute MCS % between two molecules\n",
    "def compute_mcs_percent(mol1, mol2):\n",
    "    mcs = rdFMCS.FindMCS([mol1, mol2], completeRingsOnly=True)\n",
    "    if mcs.smartsString == \"\":\n",
    "        return 0.0\n",
    "    common = Chem.MolFromSmarts(mcs.smartsString)\n",
    "    return (common.GetNumAtoms() / mol1.GetNumAtoms()) * 100\n",
    "\n",
    "# Precompute all spectra into Spectrum objects\n",
    "def prepare_spectra(df):\n",
    "    spectra = []\n",
    "    for idx, row in df.iterrows():\n",
    "        spectra.append(\n",
    "            Spectrum(mz=row[\"mzs\"],\n",
    "                     intensities=row[\"intensities\"],\n",
    "                     metadata={\"precursor_mz\": row[\"precursor_mz\"]})\n",
    "        )\n",
    "    return spectra\n",
    "\n",
    "# Precompute all molecules into RDKit Mol objects\n",
    "def prepare_mols(df):\n",
    "    mols = []\n",
    "    for idx, row in df.iterrows():\n",
    "        mols.append(Chem.MolFromSmiles(row[\"smiles\"]))\n",
    "    return mols\n",
    "\n",
    "# Main function\n",
    "def fast_similarity_search(df, top_k=5):\n",
    "    # Step 1: Prepare spectra and mols\n",
    "    print(\"Preparing spectra and molecules...\")\n",
    "    spectra = prepare_spectra(df)\n",
    "    mols = prepare_mols(df)\n",
    "    \n",
    "    # Step 2: Precompute matchms similarities\n",
    "    print(\"Computing spectral similarities...\")\n",
    "    matchms_matrix = np.zeros((len(df), len(df)))\n",
    "    for i in tqdm(range(len(spectra))):\n",
    "        for j in range(i+1, len(spectra)):\n",
    "            score = similarity_function(spectra[i], spectra[j])\n",
    "            matchms_matrix[i, j] = score\n",
    "            matchms_matrix[j, i] = score  # symmetric\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Step 3: For each spectrum, find top K candidates\n",
    "    print(\"Selecting best candidates...\")\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        similarities = matchms_matrix[idx]\n",
    "        # Ignore self-comparison\n",
    "        similarities[idx] = -1.0\n",
    "        # Find top K indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:]\n",
    "        \n",
    "        best_score = -1\n",
    "        best_candidate_idx = None\n",
    "        best_matchms = None\n",
    "        best_mcs = None\n",
    "\n",
    "        for candidate_idx in top_indices:\n",
    "            sim = similarities[candidate_idx]\n",
    "            if sim < 0.3:  # Optional: minimal similarity threshold\n",
    "                continue\n",
    "\n",
    "            # Compute MCS %\n",
    "            mcs_percent = compute_mcs_percent(mols[idx], mols[candidate_idx])\n",
    "\n",
    "            combined_score = min(sim, mcs_percent / 100)\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_candidate_idx = candidate_idx\n",
    "                best_matchms = sim\n",
    "                best_mcs = mcs_percent\n",
    "\n",
    "        if best_candidate_idx is not None:\n",
    "            results.append({\n",
    "                \"query_inchikey\": df.iloc[idx][\"inchikey\"],\n",
    "                \"similar_inchikey\": df.iloc[best_candidate_idx][\"inchikey\"],\n",
    "                \"matchms_similarity\": best_matchms,\n",
    "                \"mcs_percent\": best_mcs,\n",
    "                \"combined_score\": best_score\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"query_inchikey\": df.iloc[idx][\"inchikey\"],\n",
    "                \"similar_inchikey\": None,\n",
    "                \"matchms_similarity\": None,\n",
    "                \"mcs_percent\": None,\n",
    "                \"combined_score\": None\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matchms import Spectrum\n",
    "from matchms.similarity import ModifiedCosine\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to compute matchms similarity between two spectra\n",
    "def similarity_function(spectrum_1, spectrum_2):\n",
    "    modified_cosine = ModifiedCosine(tolerance=0.01)\n",
    "    score = modified_cosine.pair(spectrum_1, spectrum_2)\n",
    "    return float(score['score'])\n",
    "\n",
    "# Function to compute MCS % between two molecules\n",
    "def compute_mcs_percent(mol1, mol2):\n",
    "    mcs = rdFMCS.FindMCS([mol1, mol2], completeRingsOnly=True)\n",
    "    if mcs.smartsString == \"\":\n",
    "        return 0.0\n",
    "    common = Chem.MolFromSmarts(mcs.smartsString)\n",
    "    return (common.GetNumAtoms() / mol1.GetNumAtoms()) * 100\n",
    "\n",
    "# Precompute all spectra into Spectrum objects\n",
    "def prepare_spectra(df):\n",
    "    spectra = []\n",
    "    for idx, row in df.iterrows():\n",
    "        spectra.append(\n",
    "            Spectrum(mz=row[\"mzs\"],\n",
    "                     intensities=row[\"intensities\"],\n",
    "                     metadata={\"precursor_mz\": row[\"precursor_mz\"]})\n",
    "        )\n",
    "    return spectra\n",
    "\n",
    "# Precompute all molecules into RDKit Mol objects\n",
    "def prepare_mols(df):\n",
    "    mols = []\n",
    "    for idx, row in df.iterrows():\n",
    "        mols.append(Chem.MolFromSmiles(row[\"smiles\"]))\n",
    "    return mols\n",
    "\n",
    "# Main function\n",
    "def fast_similarity_search(df, top_k=5):\n",
    "    # Step 1: Prepare spectra and mols\n",
    "    print(\"Preparing spectra and molecules...\")\n",
    "    spectra = prepare_spectra(df)\n",
    "    mols = prepare_mols(df)\n",
    "    \n",
    "    # Step 2: Precompute matchms similarities\n",
    "    print(\"Computing spectral similarities...\")\n",
    "    matchms_matrix = np.zeros((len(df), len(df)))\n",
    "    for i in tqdm(range(len(spectra))):\n",
    "        for j in range(i+1, len(spectra)):\n",
    "            score = similarity_function(spectra[i], spectra[j])\n",
    "            matchms_matrix[i, j] = score\n",
    "            matchms_matrix[j, i] = score  # symmetric\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Step 3: For each spectrum, find top K candidates\n",
    "    print(\"Selecting best candidates...\")\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        similarities = matchms_matrix[idx]\n",
    "        # Ignore self-comparison\n",
    "        similarities[idx] = -1.0\n",
    "        # Find top K indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:]\n",
    "        \n",
    "        best_score = -1\n",
    "        best_candidate_idx = None\n",
    "        best_matchms = None\n",
    "        best_mcs = None\n",
    "\n",
    "        for candidate_idx in top_indices:\n",
    "            sim = similarities[candidate_idx]\n",
    "            if sim < 0.3:  # Optional: minimal similarity threshold\n",
    "                continue\n",
    "\n",
    "            # Compute MCS %\n",
    "            mcs_percent = compute_mcs_percent(mols[idx], mols[candidate_idx])\n",
    "\n",
    "            combined_score = min(sim, mcs_percent / 100)\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_candidate_idx = candidate_idx\n",
    "                best_matchms = sim\n",
    "                best_mcs = mcs_percent\n",
    "\n",
    "        if best_candidate_idx is not None:\n",
    "            results.append({\n",
    "                \"query_inchikey\": df.iloc[idx][\"inchikey\"],\n",
    "                \"similar_inchikey\": df.iloc[best_candidate_idx][\"inchikey\"],\n",
    "                \"matchms_similarity\": best_matchms,\n",
    "                \"mcs_percent\": best_mcs,\n",
    "                \"combined_score\": best_score\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"query_inchikey\": df.iloc[idx][\"inchikey\"],\n",
    "                \"similar_inchikey\": None,\n",
    "                \"matchms_similarity\": None,\n",
    "                \"mcs_percent\": None,\n",
    "                \"combined_score\": None\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matchms import Spectrum\n",
    "from matchms.similarity import ModifiedCosine\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def similarity_function(spectrum_1, spectrum_2):\n",
    "    modified_cosine = ModifiedCosine(tolerance=0.01)\n",
    "    score = modified_cosine.pair(spectrum_1, spectrum_2)\n",
    "    return float(score['score'])\n",
    "\n",
    "def compute_mcs_percent(mol1, mol2):\n",
    "    mcs = rdFMCS.FindMCS([mol1, mol2], completeRingsOnly=True)\n",
    "    if mcs.smartsString == \"\":\n",
    "        return 0.0\n",
    "    common = Chem.MolFromSmarts(mcs.smartsString)\n",
    "    return (common.GetNumAtoms() / mol1.GetNumAtoms()) * 100\n",
    "\n",
    "def process_single_query(idx, df, spectra, mols, top_k=5):\n",
    "    ref_spectrum = spectra[idx]\n",
    "    ref_mol = mols[idx]\n",
    "    ref_inchikey = df.iloc[idx][\"inchikey\"]\n",
    "\n",
    "    similarities = []\n",
    "    for j in range(len(df)):\n",
    "        if j == idx:\n",
    "            continue\n",
    "        sim = similarity_function(ref_spectrum, spectra[j])\n",
    "        similarities.append((j, sim))\n",
    "\n",
    "    # Sort and pick top_k\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    best_score = -1\n",
    "    best_candidate_idx = None\n",
    "    best_matchms = None\n",
    "    best_mcs = None\n",
    "\n",
    "    for candidate_idx, sim in similarities:\n",
    "        if sim < 0.3:\n",
    "            continue\n",
    "        mcs_percent = compute_mcs_percent(ref_mol, mols[candidate_idx])\n",
    "        combined_score = min(sim, mcs_percent / 100)\n",
    "\n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_candidate_idx = candidate_idx\n",
    "            best_matchms = sim\n",
    "            best_mcs = mcs_percent\n",
    "\n",
    "    if best_candidate_idx is not None:\n",
    "        return {\n",
    "            \"query_inchikey\": ref_inchikey,\n",
    "            \"similar_inchikey\": df.iloc[best_candidate_idx][\"inchikey\"],\n",
    "            \"matchms_similarity\": best_matchms,\n",
    "            \"mcs_percent\": best_mcs,\n",
    "            \"combined_score\": best_score\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"query_inchikey\": ref_inchikey,\n",
    "            \"similar_inchikey\": None,\n",
    "            \"matchms_similarity\": None,\n",
    "            \"mcs_percent\": None,\n",
    "            \"combined_score\": None\n",
    "        }\n",
    "\n",
    "def full_similarity_search_parallel(df, top_k=5, n_jobs=8):\n",
    "    print(\"Preparing spectra and mols...\")\n",
    "    spectra = [\n",
    "        Spectrum(mz=row[\"mzs\"], intensities=row[\"intensities\"], metadata={\"precursor_mz\": row[\"precursor_mz\"]})\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    mols = [Chem.MolFromSmiles(row[\"smiles\"]) for _, row in df.iterrows()]\n",
    "\n",
    "    print(\"Running full parallel search...\")\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_single_query)(i, df, spectra, mols, top_k)\n",
    "        for i in tqdm(range(len(df)))\n",
    "    )\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing spectra and mols...\n",
      "Running full parallel search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 96/17220 [01:30<6:02:23,  1.27s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m full_similarity_search_parallel(df_train, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      2\u001b[0m results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_results_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 80\u001b[0m, in \u001b[0;36mfull_similarity_search_parallel\u001b[0;34m(df, top_k, n_jobs)\u001b[0m\n\u001b[1;32m     77\u001b[0m mols \u001b[38;5;241m=\u001b[39m [Chem\u001b[38;5;241m.\u001b[39mMolFromSmiles(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning full parallel search...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)(\n\u001b[1;32m     81\u001b[0m     delayed(process_single_query)(i, df, spectra, mols, top_k)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)))\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = full_similarity_search_parallel(df_train, top_k=5, n_jobs=8)\n",
    "results.to_csv(\"similarity_results_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to .mgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms import Spectrum\n",
    "from matchms.exporting import save_as_mgf\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def to_1d_array(x):\n",
    "    \"\"\"Safely convert any array-like input to a clean 1D numpy array.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            x = json.loads(x)\n",
    "        except Exception:\n",
    "            x = [float(x)]\n",
    "    x = np.array(x).flatten()  # ← flatten ensures 1D structure\n",
    "    return x\n",
    "\n",
    "# Convert spectra to MGF format\n",
    "def convert_to_mgf(row, fold=\"train\"):\n",
    "    \"\"\"\n",
    "    Convert a single row of spectral data into MGF format and save it as a file.\n",
    "\n",
    "    Parameters:\n",
    "    row (pd.Series): A row from the DataFrame containing spectral data. It must include\n",
    "                     the columns 'mzs', 'intensities', 'identifier', 'inchikey', 'smiles',\n",
    "                     and 'precursor_mz'.\n",
    "    fold (str): The folder name where the MGF file will be saved. Default is \"train\".\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    mzs = to_1d_array(row[\"mzs\"])\n",
    "    intensities = to_1d_array(row[\"intensities\"])\n",
    "    identifier = row[\"identifier\"]\n",
    "    inchikey = row[\"inchikey\"]\n",
    "    smiles = row[\"smiles\"]\n",
    "    precursor_mz = row[\"precursor_mz\"]\n",
    "\n",
    "    # Create a spectrum\n",
    "    spectrum = Spectrum(\n",
    "        mz=mzs,\n",
    "        intensities=intensities,\n",
    "        metadata={\n",
    "            \"id\": inchikey,\n",
    "            \"identifier\": identifier,\n",
    "            \"inchikey\": inchikey,\n",
    "            \"smiles\": smiles,\n",
    "            \"precursor_mz\": precursor_mz,\n",
    "            \"charge\": 1,\n",
    "            \"retention_time\": 0.0,\n",
    "            \"rt\": 0.0\n",
    "        },\n",
    "    )\n",
    "    # Save as .mgf\n",
    "    save_as_mgf(spectrum, f\"{fold}/{inchikey}.mgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "2993    None\n",
       "2994    None\n",
       "2995    None\n",
       "2996    None\n",
       "2997    None\n",
       "Length: 2998, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.apply(lambda row: convert_to_mgf(row, fold=\"train\"), axis=1)\n",
    "df_val.apply(lambda row: convert_to_mgf(row, fold=\"val\"), axis=1)\n",
    "df_test.apply(lambda row: convert_to_mgf(row, fold=\"test\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folders = [\"train\", \"test\", \"val\"]\n",
    "for folder in folders:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".mgf\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            with open(filepath, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Заменяем строки\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                if line.strip().startswith(\"RETENTION_TIME=\"):\n",
    "                    value = line.strip().split(\"=\", 1)[1]\n",
    "                    new_lines.append(f\"rt={value}\\n\")  # 👈 меняем на \"rt=\"\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            # Перезаписываем файл\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
